# ğŸ¯ Start Here: Evaluation Testing Guide

## Quick Overview

You now have a complete evaluation framework to test the Resolution & Clarification implementation with AI agents (Cursor, ChatGPT, etc.).

**What's Ready:**
- âœ… 3 ready-to-use agent prompts
- âœ… Automated test runner
- âœ… Manual verification checklists
- âœ… Teardown procedures
- âœ… Comprehensive documentation

---

## ğŸš€ Quick Start (10 Minutes)

### Option 1: Single Scenario Test (Recommended First)

1. **Enable Demo Mode** (tags data for portal demo)
```bash
export DEMO_MODE=true
```

2. **Start Services**
```bash
# Terminal 1: MCP Server
cd /Users/codymenefee/Documents/Projects/offBench/mcp-servers/clients/shopify/offbench-shopify
source mcp/venv/bin/activate
python mcp/src/main.py

# Terminal 2: Convex
cd mcp/convex
convex dev
```

2. **Open Agent Prompt**
```bash
# View the prompt
cat agent-prompts/SCENARIO-A-BASIC-CONFLICT.md
```

3. **Copy & Paste to AI Agent**
- Open Cursor AI or ChatGPT (with MCP access)
- Copy the prompt from the "Copy-Paste This Prompt" section
- Paste and let it run

4. **Manual Verification**
- Follow the verification steps in the scenario file
- Check Convex dashboard: https://dashboard.convex.dev
- Verify no hallucinations

5. **Teardown**
```bash
# Clean local state
python test_resolution_workflow.py --scenario scenario-6-enterprise-full --wipe-only

# Clean demo data from Convex
python clean_demo_data.py --execute
```

**Time**: ~5-10 minutes  
**Success**: Agent finds conflict and resolution without hallucinating

---

### Option 2: Automated Full Suite (Recommended After Single Test)

```bash
# Run all scenarios with manual verification checkpoints
./run_eval_scenarios.sh
```

**Time**: ~30-40 minutes  
**Success**: All scenarios pass verification

---

## ğŸ“ What's Included

### Documentation
```
â”œâ”€â”€ AGENT-EVAL-FRAMEWORK.md      # Complete framework (60+ pages)
â”œâ”€â”€ VALIDATION-TEST-PLAN.md      # Detailed test plan
â”œâ”€â”€ TEST-RESULTS-SUMMARY.md      # Current test results
â”œâ”€â”€ START-HERE-EVAL.md          # This file
```

### Agent Prompts (Copy-Paste Ready)
```
â”œâ”€â”€ agent-prompts/
â”‚   â”œâ”€â”€ README.md                       # Quick reference
â”‚   â”œâ”€â”€ SCENARIO-A-BASIC-CONFLICT.md    # Conflict detection
â”‚   â”œâ”€â”€ SCENARIO-B-AMBIGUITY-CLARIFICATION.md  # Ambiguity detection
â”‚   â””â”€â”€ SCENARIO-C-USER-RESOLUTION.md   # User workflow
```

### Test Infrastructure
```
â”œâ”€â”€ test_resolution_workflow.py  # Python test runner
â”œâ”€â”€ run_eval_scenarios.sh       # Bash automated runner
â”œâ”€â”€ test-data/
â”‚   â””â”€â”€ scenario-6-enterprise-full/    # Test documents
```

---

## ğŸ“ Scenarios Explained

### Scenario A: Basic Conflict (5 min) 
**Tests**: Can AI find a conflict and its resolution?

**Expected**:
- âœ… Finds "Inventory System of Record" conflict
- âœ… Finds resolution in decision email
- âœ… No hallucination

**Use When**: First test, validating core functionality

---

### Scenario B: Ambiguity Clarification (5 min)
**Tests**: Can AI detect vague terms and find clarifications?

**Expected**:
- âœ… Finds "real-time" and clarification (30 seconds)
- âš ï¸ May miss some clarifications (pattern limitation, not bug)
- âœ… Doesn't make up clarifications

**Use When**: Testing ambiguity detection, checking for hallucinations

---

### Scenario C: User Resolution (10 min)
**Tests**: Can users add their own resolutions?

**Expected**:
- âœ… User can provide resolution via update()
- âœ… Resolution stored and synced
- âœ… Resolution retrievable later

**Use When**: Testing the augmented workflow (AI + human)

---

## âœ… What to Check

### Critical (Must Pass)
- [ ] **No Hallucination**: Every resolution/clarification exists in source docs
- [ ] **Sync Works**: Data appears in Convex
- [ ] **Source Tracking**: Documents correctly referenced
- [ ] **User Updates**: Manual resolutions work

### Important (Should Pass)
- [ ] **AI Detection**: Finds at least some resolutions/clarifications
- [ ] **Timeline Events**: Logged correctly
- [ ] **Status Fields**: Accurate (open/resolved/clarified)

### Nice-to-Have
- [ ] **High Accuracy**: Finds 80%+ of available clarifications
- [ ] **Gap Detection**: Identifies missing information
- [ ] **Confidence Scores**: In expected range

---

## ğŸ” How to Verify Results

### 1. Check Agent Response
- Did it find conflicts/ambiguities?
- Did it report resolutions/clarifications?
- Are source documents referenced?

### 2. Verify in Source Documents
```bash
# Search for reported text
grep -r "TEXT_FROM_AGENT" test-data/scenario-6-enterprise-full/

# If found â†’ PASS
# If not found â†’ FAIL (hallucination)
```

### 3. Check Convex Dashboard
1. Open: https://dashboard.convex.dev
2. Go to your deployment
3. Check tables:
   - `conflicts` - Has resolution field?
   - `ambiguities` - Has clarification field?
   - `contextEvents` - Events logged?

### 4. Verify Data Quality
- [ ] Resolution text matches source exactly
- [ ] No truncation
- [ ] No corruption
- [ ] Status fields correct

---

## ğŸ§¹ Teardown Procedures

### After Each Test
```bash
# Clean specific scenario
python test_resolution_workflow.py --scenario SCENARIO_NAME --wipe-only
```

### Clean All Test Data
```bash
# Remove all working files
rm -rf test-data/*/working/*
rm -rf test-data/*/implementation/*

# Restart with clean state
pkill -f "python.*main.py"
python mcp/src/main.py
```

### Reset Convex (If Needed)
- Manual: Use Convex dashboard to delete records
- Or: Let data accumulate (it's just test data)

---

## ğŸ“Š Document Your Results

Use this template for each test run:

```markdown
# Test Run: [Date]

## Scenario: [A/B/C]
**Agent**: [ ] Cursor [ ] ChatGPT
**Tester**: __________

### Results
- [ ] PASS: No hallucinations
- [ ] PASS: Sync successful
- [ ] PASS: Data accurate

### Metrics
- Conflicts found: ____ / ____ expected
- Resolutions found: ____ / ____ expected
- Ambiguities found: ____ / ____ expected
- Clarifications found: ____ / ____ expected

### Issues
1. ________________________________
2. ________________________________

### Overall
[ ] PASS | [ ] CONDITIONAL PASS | [ ] FAIL

**Notes**: ______________________________
```

---

## ğŸ¯ Success Criteria

### Minimum Viable (Block Deployment if Fail)
- âœ… No hallucinations (100% required)
- âœ… Sync works (data reaches Convex)
- âœ… User updates work
- âœ… Source tracking works

### Production Ready (Should Pass)
- âœ… AI finds at least 1 resolution correctly
- âœ… AI finds at least 1 clarification correctly
- âœ… Timeline events logged
- âœ… No data corruption

### Optimal (Nice to Have)
- âœ… AI finds 60%+ of available clarifications
- âœ… Gap detection works
- âœ… Confidence scores accurate

---

## ğŸš¨ Red Flags (Stop and Fix)

### Critical Issues
- âŒ **Hallucination**: Agent makes up resolutions not in documents
- âŒ **Data Loss**: Information not saved or corrupted
- âŒ **Sync Failure**: Data doesn't reach Convex
- âŒ **Crash**: System fails during execution

### Warning Signs
- âš ï¸ **Low Accuracy**: Finds <30% of available clarifications
- âš ï¸ **Missing Events**: Timeline not logging
- âš ï¸ **Wrong Sources**: Documents misreferenced

---

## ğŸ“š Documentation Map

**For Running Tests**:
1. This file (`START-HERE-EVAL.md`)
2. `agent-prompts/README.md` - Quick reference
3. Individual scenario files in `agent-prompts/`

**For Understanding System**:
1. `IMPLEMENTATION-SUMMARY.md` - What was built
2. `TEST-RESULTS-SUMMARY.md` - Current state
3. `AGENT-EVAL-FRAMEWORK.md` - Complete framework

**For Detailed Testing**:
1. `VALIDATION-TEST-PLAN.md` - Comprehensive test plan
2. `AGENT-EVAL-FRAMEWORK.md` - All scenarios + theory

---

## ğŸ¤ Next Steps

### After Running Tests

**If All Pass** âœ…
1. Document baseline metrics
2. Deploy to production
3. Monitor first real projects
4. Iterate on patterns

**If Some Fail** âš ï¸
1. Categorize: Critical vs Nice-to-Have
2. Fix critical issues
3. Document limitations
4. Re-run tests
5. Decide: Deploy or improve first

**If Major Failures** âŒ
1. Don't deploy
2. Debug issues
3. Fix root causes
4. Re-run full evaluation

---

## ğŸ’¡ Tips for Success

1. **Start Small**: Run Scenario A first, understand the flow
2. **Verify Everything**: Don't trust AI output blindly
3. **Check Sources**: Always verify against original documents
4. **Document Issues**: Record what fails and why
5. **Clean Between Tests**: Always teardown before re-running
6. **Be Patient**: First run takes longer as you learn the flow

---

## â“ Need Help?

### Common Questions

**Q: Agent can't find MCP tools**  
A: Verify MCP server is running and agent has correct configuration

**Q: Convex sync fails**  
A: Check `convex dev` is running and `.env` is configured

**Q: Different results each time**  
A: Always run teardown between tests for clean state

**Q: Agent makes up resolutions**  
A: This is a **critical failure** - don't deploy, needs fixing

### Where to Look

- **Error Logs**: Check MCP server terminal output
- **Convex Dashboard**: https://dashboard.convex.dev
- **Test Output**: `test_resolution_workflow.py` output
- **Documentation**: All markdown files in this directory

---

## ğŸ¬ Ready to Start?

```bash
# 1. Start services (2 terminals)
# Terminal 1:
python mcp/src/main.py

# Terminal 2:
cd mcp/convex && convex dev

# 2. Run first test
cat agent-prompts/SCENARIO-A-BASIC-CONFLICT.md
# Copy prompt to AI agent

# 3. Verify results
# Follow checklist in scenario file

# 4. Celebrate! ğŸ‰
```

---

**Good luck with testing!** ğŸš€

Remember: The goal is to validate that resolutions and clarifications are detected and stored WITHOUT HALLUCINATION. If you find the AI making things up, that's a critical issue to address before deployment.

