# ğŸ“‹ Evaluation Framework - Complete Index

## ğŸ¯ What You Have

A complete evaluation framework for testing Resolution & Clarification support with AI agents.

---

## ğŸš€ Start Here

**New to this?** â†’ Read: `START-HERE-EVAL.md`

**Quick test (5 min)?** â†’ Use: `agent-prompts/SCENARIO-A-BASIC-CONFLICT.md`

**Full evaluation (45 min)?** â†’ Run: `./run_eval_scenarios.sh`

---

## ğŸ“ Complete File Structure

```
offbench-shopify/
â”‚
â”œâ”€â”€ START-HERE-EVAL.md               â­ START HERE - Quick start guide
â”œâ”€â”€ EVALUATION-INDEX.md              ğŸ“‹ This file - Complete index
â”‚
â”œâ”€â”€ Documentation/
â”‚   â”œâ”€â”€ AGENT-EVAL-FRAMEWORK.md      ğŸ“š Complete framework (comprehensive)
â”‚   â”œâ”€â”€ VALIDATION-TEST-PLAN.md      âœ… Detailed test plan
â”‚   â”œâ”€â”€ TEST-RESULTS-SUMMARY.md      ğŸ“Š Current test results & findings
â”‚   â”œâ”€â”€ IMPLEMENTATION-SUMMARY.md    ğŸ”§ What was built
â”‚   â””â”€â”€ TEST-QUICK-START.md          âš¡ Quick testing guide
â”‚
â”œâ”€â”€ Agent Prompts/ (Copy-Paste Ready)
â”‚   â”œâ”€â”€ agent-prompts/README.md                      ğŸ“– Quick reference
â”‚   â”œâ”€â”€ agent-prompts/SCENARIO-A-BASIC-CONFLICT.md   âœ… Test 1: Conflict detection (5 min)
â”‚   â”œâ”€â”€ agent-prompts/SCENARIO-B-AMBIGUITY-CLARIFICATION.md  âœ… Test 2: Clarifications (5 min)
â”‚   â””â”€â”€ agent-prompts/SCENARIO-C-USER-RESOLUTION.md  âœ… Test 3: User workflow (10 min)
â”‚
â”œâ”€â”€ Test Infrastructure/
â”‚   â”œâ”€â”€ test_resolution_workflow.py  ğŸ Python test runner
â”‚   â”œâ”€â”€ run_eval_scenarios.sh       ğŸ”„ Automated test suite
â”‚   â””â”€â”€ test-data/
â”‚       â””â”€â”€ scenario-6-enterprise-full/   ğŸ“¦ Test documents (7 files)
â”‚           â”œâ”€â”€ emails/              âœ‰ï¸ Decision & clarification emails
â”‚           â”œâ”€â”€ transcripts/         ğŸ¤ Sales & technical calls
â”‚           â”œâ”€â”€ client-docs/         ğŸ“„ RFP & brand guidelines
â”‚           â””â”€â”€ expected-results.json ğŸ¯ Validation criteria
â”‚
â””â”€â”€ Implementation/
    â”œâ”€â”€ mcp/convex/schema.ts         âœ… Updated schema
    â”œâ”€â”€ mcp/src/models/analysis.py   âœ… Updated models
    â”œâ”€â”€ mcp/src/core/analyzer.py     âœ… Detection logic
    â””â”€â”€ mcp/src/persistence/convex_sync.py  âœ… Sync operations
```

---

## ğŸ¯ Three Ways to Test

### Method 1: Manual AI Agent Testing (Recommended)
**Best for**: Understanding the system, manual verification

1. Start MCP server
2. Open agent prompt file
3. Copy prompt to AI agent (Cursor/ChatGPT)
4. Watch agent execute
5. Manually verify results
6. Document findings

**Time**: 5-10 min per scenario  
**Files**: `agent-prompts/SCENARIO-*.md`

---

### Method 2: Automated Python Testing
**Best for**: Quick validation, regression testing

```bash
python test_resolution_workflow.py --run-full
```

**Time**: 2 minutes  
**Files**: `test_resolution_workflow.py`

---

### Method 3: Full Automated Suite
**Best for**: Complete evaluation, batch testing

```bash
./run_eval_scenarios.sh
```

**Time**: 30-40 minutes  
**Files**: `run_eval_scenarios.sh`

---

## ğŸ“– Reading Order

### For Quick Start (10 min)
1. `START-HERE-EVAL.md` - Overview
2. `agent-prompts/SCENARIO-A-BASIC-CONFLICT.md` - First test
3. Run the test!

### For Complete Understanding (45 min)
1. `START-HERE-EVAL.md` - Overview
2. `IMPLEMENTATION-SUMMARY.md` - What was built
3. `TEST-RESULTS-SUMMARY.md` - Current state
4. `agent-prompts/README.md` - Test scenarios
5. Run all tests
6. `AGENT-EVAL-FRAMEWORK.md` - Deep dive

### For Comprehensive Testing (2 hours)
1. All of the above
2. `VALIDATION-TEST-PLAN.md` - Detailed test plan
3. Run manual verification for each scenario
4. Document results thoroughly

---

## âœ… What Each Document Does

| Document | Purpose | When to Use |
|----------|---------|-------------|
| `START-HERE-EVAL.md` | Quick start guide | First time setup |
| `EVALUATION-INDEX.md` | Navigation (this file) | Finding your way around |
| `AGENT-EVAL-FRAMEWORK.md` | Complete framework | Deep understanding |
| `VALIDATION-TEST-PLAN.md` | Detailed test procedures | Comprehensive testing |
| `TEST-RESULTS-SUMMARY.md` | Current test results | Understanding status |
| `IMPLEMENTATION-SUMMARY.md` | Technical changes | Understanding code |
| `agent-prompts/README.md` | Prompt quick reference | Running tests |
| `agent-prompts/SCENARIO-*.md` | Copy-paste prompts | Actual testing |

---

## ğŸ¯ Test Scenarios Summary

### Scenario A: Basic Conflict Detection (5 min)
**File**: `agent-prompts/SCENARIO-A-BASIC-CONFLICT.md`

**Tests**:
- âœ… Conflict detection
- âœ… Resolution finding
- âœ… No hallucination
- âœ… Source tracking

**Expected**: Finds inventory conflict with resolution

---

### Scenario B: Ambiguity Clarification (5 min)
**File**: `agent-prompts/SCENARIO-B-AMBIGUITY-CLARIFICATION.md`

**Tests**:
- âœ… Ambiguity detection
- âœ… Clarification finding
- âœ… Handles missing clarifications
- âœ… No hallucination

**Expected**: Finds "real-time" clarification (30 seconds)

---

### Scenario C: User Resolution Workflow (10 min)
**File**: `agent-prompts/SCENARIO-C-USER-RESOLUTION.md`

**Tests**:
- âœ… User can add resolutions
- âœ… Data persists to Convex
- âœ… Resolutions retrievable
- âœ… Timeline events logged

**Expected**: User resolution stored and synced

---

## ğŸ› ï¸ Quick Commands Reference

### Start Services
```bash
# Terminal 1: MCP Server
python mcp/src/main.py

# Terminal 2: Convex
cd mcp/convex && convex dev
```

### Run Tests
```bash
# Single automated test
python test_resolution_workflow.py --run-full

# Full suite with checkpoints
./run_eval_scenarios.sh

# Specific scenario
python test_resolution_workflow.py --scenario scenario-6-enterprise-full --run-full
```

### Teardown
```bash
# Clean specific scenario
python test_resolution_workflow.py --scenario SCENARIO_NAME --wipe-only

# Clean all
rm -rf test-data/*/working/* test-data/*/implementation/*
```

### Verify
```bash
# Check for hallucination
grep -r "REPORTED_TEXT" test-data/scenario-6-enterprise-full/

# Check Convex
open https://dashboard.convex.dev

# Check MCP server running
ps aux | grep main.py
```

---

## ğŸ“Š Success Criteria

### Must Pass (Blocking)
- [ ] No hallucinations (resolutions/clarifications exist in docs)
- [ ] Convex sync works
- [ ] User updates work
- [ ] No data corruption

### Should Pass (Important)
- [ ] AI finds at least 1 resolution
- [ ] AI finds at least 1 clarification
- [ ] Timeline events logged
- [ ] Source tracking accurate

### Nice to Have
- [ ] 60%+ clarification detection accuracy
- [ ] Gap detection works
- [ ] Confidence scores accurate

---

## ğŸš¨ Red Flags

**Stop and fix if you see:**
- âŒ Hallucinated resolutions/clarifications
- âŒ Data not syncing to Convex
- âŒ Data corruption or loss
- âŒ System crashes

**Warning signs (investigate):**
- âš ï¸ Very low accuracy (<30%)
- âš ï¸ Missing timeline events
- âš ï¸ Wrong source documents

---

## ğŸ’¡ Pro Tips

1. **Always start with Scenario A** - simplest test, validates core
2. **Check Convex after each test** - verify data is there
3. **Search for reported text in docs** - catch hallucinations
4. **Clean between runs** - ensures consistent results
5. **Document everything** - you'll want the baseline later

---

## ğŸ“ Next Steps

### After First Test
- [ ] Read test results
- [ ] Check Convex dashboard
- [ ] Verify no hallucinations
- [ ] Document findings

### After All Tests
- [ ] Review success criteria
- [ ] Categorize any failures
- [ ] Decide: Deploy, Improve, or Iterate
- [ ] Update baseline metrics

### For Production
- [ ] All tests pass
- [ ] Documentation complete
- [ ] Baseline metrics established
- [ ] Monitoring plan ready

---

## ğŸ“ Learning Path

**Beginner** (30 min):
1. Read `START-HERE-EVAL.md`
2. Run Scenario A
3. Verify results manually

**Intermediate** (90 min):
1. All beginner steps
2. Run all 3 scenarios
3. Read `TEST-RESULTS-SUMMARY.md`
4. Document findings

**Advanced** (3+ hours):
1. All intermediate steps
2. Read `AGENT-EVAL-FRAMEWORK.md`
3. Run automated suite
4. Create custom scenarios
5. Performance testing

---

## ğŸ“š Additional Resources

**Technical Deep Dive**:
- `mcp/convex/schema.ts` - Database schema
- `mcp/src/models/analysis.py` - Data models
- `mcp/src/core/analyzer.py` - Detection logic

**Test Data**:
- `test-data/scenario-6-enterprise-full/` - Documents
- `test-data/scenario-6-enterprise-full/expected-results.json` - Criteria

**Configuration**:
- `mcp/.env` - Environment variables
- `mcp/convex/convex.json` - Convex config

---

## âœ¨ You're All Set!

Everything you need to validate the Resolution & Clarification implementation is ready.

**Quick Start**:
```bash
# 1. Open the guide
open START-HERE-EVAL.md

# 2. Pick a scenario
open agent-prompts/SCENARIO-A-BASIC-CONFLICT.md

# 3. Start testing!
```

---

**Questions?** Check the relevant documentation file above.

**Ready to test?** Start with `START-HERE-EVAL.md`

**Good luck! ğŸš€**
